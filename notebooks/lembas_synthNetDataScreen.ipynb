{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to reproduce the `Model/synthNetDataScreen.py` script from Avlant's work\n",
    "\n",
    "\n",
    "It was run on the cluster. The args were found in the `sunSlurmSynthScreen.sh` file. It takes a value between 0 and 14. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from nn_cno import nn_models\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import jax.tree_util as jtu\n",
    "from jax.experimental import sparse\n",
    "from jax import numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import functools as ft\n",
    "\n",
    "# import argparse\n",
    "# This code is used to evaluate the different parts on a cluster. \n",
    "\n",
    "# #Get data number\n",
    "# parser = argparse.ArgumentParser(prog='Macrophage simulation')\n",
    "# parser.add_argument('--selectedCondition', action='store', default=None)\n",
    "# args = parser.parse_args()\n",
    "# curentId = int(args.selectedCondition)\n",
    "\n",
    "currentID = 0     # goes between 0-14 based on the sunSlurmSynthScreen.sh file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the model using the new implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testCondtions = pd.read_csv('synthNetScreen/conditions.tsv', sep='\\t', low_memory=False)\n",
    "simultaniousInput = int(testCondtions.loc[currentID == testCondtions['Index'],:]['Ligands'].values)\n",
    "N = int(testCondtions.loc[currentID == testCondtions['Index'],:]['DataSize'].values)\n",
    "print(currentID, simultaniousInput, N)\n",
    "\n",
    "inputAmplitude = 3\n",
    "projectionAmplitude = 1.2\n",
    "\n",
    "modelFile = \"data/KEGGnet-Model.tsv\"\n",
    "annotationFile = 'data/KEGGnet-Annotation.tsv'\n",
    "parameterFile = 'synthNetScreen/equationParams.txt'\n",
    "\n",
    "parameterizedModel = nn_models.bioNetwork(networkFile=modelFile, \n",
    "                                          nodeAnnotationFile=annotationFile,\n",
    "                                          inputAmplitude=inputAmplitude,\n",
    "                                          projectionAmplitude=projectionAmplitude)\n",
    "parameterizedModel.loadParams(parameterFile)\n",
    "\n",
    "Model  = nn_models.bioNetwork(networkFile=modelFile, \n",
    "                                          nodeAnnotationFile=annotationFile,\n",
    "                                          inputAmplitude=inputAmplitude,\n",
    "                                          projectionAmplitude=projectionAmplitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-silico data was generated by simulating the model with given parameters.\n",
    "I think the in-silico model was a result of some kind of optimization, because the paper\n",
    "mentions that the random parameterization results in pretty uniform output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "X = np.zeros((N, len(parameterizedModel.network.inName)))\n",
    "for i in range(1, N): #skip 0 to include a ctrl sample i.e. zero input\n",
    "    X[i, (i-1) % len(parameterizedModel.network.inName)] = np.random.rand(1) #stimulate each receptor at least once\n",
    "    X[i, np.random.randint(0, len(parameterizedModel.network.inName), simultaniousInput-1)] = np.random.rand(simultaniousInput-1)\n",
    "# compared X with pytorch version\n",
    "\n",
    "controlIndex = 0\n",
    "# Y, YfullRef = parameterizedModel(X)\n",
    "Y, YfullRef = jax.vmap(parameterizedModel.model, in_axes=(0),out_axes=(0,0))(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the `dataloader_inf` for the toy model. It reuses the training data if the batch `size * n_steps` is larger than the available training data. \n",
    "\n",
    "but we should use the `dataloader_finite` for the comparison with Avlant's approach. This use each data only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch has a data loader, but jax does not.\n",
    "# trainloader = torch.utils.data.DataLoader(range(N), batch_size=batchSize, shuffle=True)\n",
    "\n",
    "# here we take the example from https://docs.kidger.site/equinox/examples/train_rnn/\n",
    "# from equinox\n",
    "\n",
    "# this is another version of dataloader, which can be emptied. \n",
    "import jax.random as jrandom\n",
    "\n",
    "# \n",
    "def dataloader_inf(arrays, batch_size, *, key):\n",
    "    \"\"\" Create an infinite dataloader from data. \n",
    "    \n",
    "    It goes through the data and returns batches of size batch_size.\n",
    "    When it finishes, it starts again from the beginning.\n",
    "\n",
    "    Args:\n",
    "        arrays: list of arrays to be batched.\n",
    "        batch_size: size of the batch.\n",
    "        key: random key.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "def dataloader_finite(arrays, batch_size, *, key):\n",
    "    \"\"\" Create an finite dataloader from data. \n",
    "    \n",
    "    It goes through the data and returns batches of size batch_size, finally, \n",
    "    it returns the last batch if the data is not divisible by batch_size.\n",
    "\n",
    "    Args:\n",
    "        arrays: list of arrays to be batched.\n",
    "        batch_size: size of the batch.\n",
    "        key: random key.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "\n",
    "    perm = jrandom.permutation(key, indices)\n",
    "    (key,) = jrandom.split(key, 1)\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    while end < dataset_size:\n",
    "        batch_perm = perm[start:end]\n",
    "        yield tuple(array[batch_perm] for array in arrays)\n",
    "        start = end\n",
    "        end = start + batch_size\n",
    "    \n",
    "    if start < dataset_size:\n",
    "        batch_perm = perm[start:]\n",
    "        yield tuple(array[batch_perm] for array in arrays)\n",
    "\n",
    "\n",
    "def dataIndexloader(arrays, batch_size, *, key):\n",
    "    \"\"\" Create the indices used in the batch.\n",
    "    \n",
    "    Args:\n",
    "        arrays: list of arrays to be batched.\n",
    "        batch_size: size of the batch.\n",
    "        key: random key.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "\n",
    "    perm = jrandom.permutation(key, indices)\n",
    "    (key,) = jrandom.split(key, 1)\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    while end < dataset_size:\n",
    "        batch_perm = perm[start:end]\n",
    "        yield batch_perm\n",
    "        start = end\n",
    "        end = start + batch_size\n",
    "    \n",
    "    if start < dataset_size:\n",
    "        batch_perm = perm[start:]\n",
    "        yield batch_perm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how to rewrite the `uniformLoss` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def uniformLoss(curState, dataIndex, YhatFull, targetMin = 0, targetMax = 0.99, maxConstraintFactor = 10):\n",
    "    #data = curState.detach().clone()\n",
    "    #data[dataIndex, :] = YhatFull\n",
    "    loss = uniformLossBatch(curState, targetMin = targetMin, targetMax = targetMax, maxConstraintFactor = maxConstraintFactor)\n",
    "    return loss\n",
    "\n",
    "#@jax.jit\n",
    "def uniformLossBatch(YhatFull, targetMin = 0, targetMax = 0.99, maxConstraintFactor = 10):\n",
    "    targetMean = (targetMax-targetMin)/2\n",
    "    targetVar = (targetMax-targetMin)**2/12\n",
    "\n",
    "    factor = 1\n",
    "    meanFactor = factor\n",
    "    varFactor = factor\n",
    "    minFactor = factor\n",
    "    maxFactor = factor\n",
    "    maxConstraintFactor = factor * maxConstraintFactor\n",
    "\n",
    "    nodeMean = jnp.mean(YhatFull, axis=0)\n",
    "    nodeVar = jnp.mean(jnp.square(YhatFull-nodeMean), axis=0)\n",
    "    maxVal = jnp.amax(YhatFull, axis=0)\n",
    "    minVal = jnp.amin(YhatFull, axis=0)\n",
    "\n",
    "    meanLoss = meanFactor * jnp.sum(jnp.square(nodeMean - targetMean))\n",
    "    varLoss =  varFactor * jnp.sum(jnp.square(nodeVar - targetVar))\n",
    "    maxLoss = maxFactor * jnp.sum(jnp.square(maxVal - targetMax))\n",
    "    minloss = minFactor * jnp.sum(jnp.square(minVal- targetMin))\n",
    "    #maxConstraint = -maxConstraintFactor * jnp.sum(maxVal[maxVal<=0]) #max value should never be negative\n",
    "    maxConstraint = -maxConstraintFactor * jnp.sum(jnp.where(maxVal<=0, maxVal, 0))\n",
    "     \n",
    "    loss = meanLoss + varLoss + minloss + maxLoss + maxConstraint\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing `uniformLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "#maxIter = 100\n",
    "loader_key = jax.random.PRNGKey(142)\n",
    "#progress = nn_models.rnnModel.OptimProgress(maxIter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 5\n",
    "# loader_key = jax.random.PRNGKey(142)\n",
    "# iter_data = dataIndexloader((X, Y), batch_size, key=loader_key)\n",
    "# dataIndex = next(iter_data)\n",
    "# x = X[dataIndex, :]\n",
    "# y = Y[dataIndex, :]\n",
    "# print(x.shape)\n",
    "# (pred_y, pred_yFull) = jax.vmap(Model.model, out_axes=(0,0))(x)\n",
    "# #currentState = jax.random.uniform(shape=(N, Model.model.layers[1].biases.shape[0]),key=statekey)\n",
    "# currentState = jnp.zeros(shape=(N, 1, Model.model.layers[1].biases.shape[0]))\n",
    "\n",
    "# print(currentState.shape)\n",
    "# print(pred_yFull.shape)\n",
    "# stateLossFactor = 1e-4\n",
    "\n",
    "# currentState = currentState.at[dataIndex,1,:].set(jnp.squeeze(pred_yFull))\n",
    "\n",
    "# # State constraints: \n",
    "# stateLoss = stateLossFactor * uniformLoss(currentState, dataIndex, pred_yFull, maxConstraintFactor = 50)\n",
    "# stateLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking spectral loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def oneStepDeltaActivationFactor(x, leak):\n",
    "    \"\"\" derivative of the activation function F(x):\n",
    "        x < 0:        leak \n",
    "        0 < x < 0.5 : 1\n",
    "        0.5 < x :      0.25/x^2\n",
    "    \"\"\"\n",
    "    y = jnp.ones(x.shape[1]) #derivative = 1 if nothing else is stated\n",
    "    y = jnp.where(x <= 0, leak, y)  #let derivative be 0.01 at x=0\n",
    "    \n",
    "    y = jnp.where(x > 0.5, 0.25/(x**2), y)\n",
    "    return y\n",
    "\n",
    "def lreig(A):\n",
    "    # for DENSE Matrix using scipy\n",
    "    #fall back if eigs fails\n",
    "    e, w, v = sp.linalg.eig(A, left = True)\n",
    "    \n",
    "    selected = np.argmax(np.abs(e))\n",
    "    eValue = e[selected]\n",
    "    w = w[:,selected]\n",
    "    v = v[:,selected]\n",
    "    \n",
    "    return eValue, v, w\n",
    "\n",
    "# A: sparse matrix\n",
    "@jax.custom_jvp\n",
    "def getspectralRadius(weights, ind, M):\n",
    "    \n",
    "    A = sp.sparse.csr_matrix((weights, ind), shape=M.shape, dtype='float32')\n",
    "    tolerance = 10**-6\n",
    "\n",
    "    try:\n",
    "        e, v = sp.sparse.linalg.eigs(A, k=1, which='LM', ncv=100, tol = tolerance)\n",
    "        v = v[:,0]\n",
    "        e = e[0]\n",
    "    except  (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "    except:\n",
    "        print('Forward failed in forward (did not find any eigenvalue with eigs)')\n",
    "        tmpA = A.toarray()\n",
    "        e, v, w = lreig(tmpA) #fall back to solving full eig problem\n",
    "\n",
    "    spectralRadius = np.abs(e)\n",
    "\n",
    "    return spectralRadius\n",
    "\n",
    "@getspectralRadius.defjvp\n",
    "def AgetspectralRadius_jvp(primals, tangents):\n",
    "    weights, ind, M = primals\n",
    "    w_dot, ind_dot, M_dot  = tangents\n",
    "\n",
    "    networkList = ind\n",
    "    A = sp.sparse.csr_matrix((weights, ind), shape=M.shape, dtype='float32')\n",
    "    tolerance = 10**-6\n",
    "    w = np.empty(0)\n",
    "\n",
    "    try:\n",
    "        e, v = sp.sparse.linalg.eigs(A, k=1, which='LM', ncv=100, tol = tolerance)\n",
    "        v = v[:,0]\n",
    "        e = e[0]\n",
    "    except  (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "    except:\n",
    "        print('Forward failed in backward (did not find any eigenvalue with eigs)')\n",
    "        tmpA = A.toarray()\n",
    "        e, v, w = lreig(tmpA) #fall back to solving full eig problem\n",
    "\n",
    "    primal_out = np.abs(e)\n",
    "    \n",
    "    tmpA = A.T \n",
    "\n",
    "    if w.shape[0]==0:\n",
    "        try:\n",
    "            eT = e\n",
    "            if np.isreal(eT): #does for some reason not converge if imag = 0\n",
    "                eT = eT.real\n",
    "            e2, w = sp.sparse.linalg.eigs(tmpA, k=1, sigma=eT, OPpart='r', tol=tolerance)\n",
    "            selected = 0 #numpy.argmin(numpy.abs(e2-eT))\n",
    "            w = w[:,selected]\n",
    "            e2 = e2[selected]\n",
    "            #Check if same eigenvalue\n",
    "            if abs(e-e2)>(tolerance*10):\n",
    "                print('Backward fail (eigs left returned different eigenvalue)')\n",
    "                w = np.empty(0)\n",
    "                #e, v, w = lreig(tmpA) #fall back to solving whole eig problem\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            print('Backward fail (did not find any eigenvalue with eigs)')\n",
    "            #e, v, w = lreig(tmpA) #fall back to solving full eig problem\n",
    "            delta = np.zeros(weights.shape)\n",
    "\n",
    "\n",
    "    if w.shape[0] != 0:\n",
    "        divisor = w.T.dot(v).flatten()\n",
    "        if abs(divisor) == 0:\n",
    "            delta = np.zeros(weights.shape)\n",
    "            print('Empty eig')\n",
    "        else:\n",
    "            delta = np.multiply(w[networkList[0]], v[networkList[1]])/divisor\n",
    "            direction = e/np.abs(e)\n",
    "            delta = (delta/direction).real\n",
    "    else:\n",
    "        #print('Empty eig')\n",
    "        delta = np.zeros(weights.shape)\n",
    "\n",
    "    constrainNorm = True\n",
    "    if constrainNorm:\n",
    "        norm = np.linalg.norm(delta)\n",
    "        if norm>10:\n",
    "            delta = delta/norm #typical seems to be ~0.36\n",
    "        #delta = delta * numpy.abs(ctx.weights)\n",
    "        #delta = delta/norm(delta)\n",
    "\n",
    "    tangent_out = jnp.dot(delta,w_dot)\n",
    "\n",
    "    return primal_out, tangent_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def spectralLoss(model, YhatFull, expFactor = 20, lb=0.5, spectralTarget =0.95):\n",
    "    \"\"\" spectral loss of the transmission function:\n",
    "    \n",
    "    check Fig3, panel A\n",
    "    derivativeOfActivation(steadystate) * A \n",
    "    \"\"\"\n",
    "\n",
    "    # selects a random condition to be used in the spectral computation:\n",
    "    randomIndex = np.random.randint(YhatFull.shape[0])\n",
    "\n",
    "    # evaluate the derivative of the activation function at the steady state:\n",
    "    activationFactor = oneStepDeltaActivationFactor(YhatFull[randomIndex,:], model.layers[1].leak)\n",
    "    weightFactor = activationFactor.flatten()[model.layers[1].networkList[0]]\n",
    "    multipliedWeightFactor =  model.layers[1].weights * weightFactor\n",
    "    \n",
    "    M = jnp.zeros(shape=model.layers[1].A.shape)\n",
    "    ind = model.layers[1].networkList\n",
    "    \n",
    "    spectralRadius = getspectralRadius(multipliedWeightFactor, ind, M)\n",
    "    \n",
    "    scaleFactor = 1/jnp.exp(expFactor *  spectralTarget)\n",
    "    \n",
    "    #spectralRadiusLoss = scaleFactor * spectralRadius\n",
    "    #spectralRadiusLoss = scaleFactor * (jnp.exp(expFactor*spectralRadius)-1)\n",
    "\n",
    "    # https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where\n",
    "    spectralRadiusLoss = jnp.where(spectralRadius > lb, \n",
    "                                   scaleFactor * (jnp.exp(expFactor*spectralRadius)-1),\n",
    "                                   jnp.zeros(1))[0]\n",
    "    # if spectralRadius>lb:\n",
    "    #     spectralRadiusLoss = scaleFactor * (jnp.exp(expFactor*spectralRadius)-1)\n",
    "    # else:\n",
    "    #     spectralRadiusLoss = 0.0\n",
    "\n",
    "    return spectralRadiusLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the spectral loss function\n",
    "# iter_data = dataIndexloader((X, Y), batch_size, key=loader_key)\n",
    "# dataIndex = next(iter_data)\n",
    "# x = X[dataIndex, :]\n",
    "# y = Y[dataIndex, :]\n",
    "# M = jnp.zeros(shape = Model.model.layers[1].A.shape)\n",
    "\n",
    "# (pred_y, pred_yFull) = jax.vmap(Model.model, out_axes=(0,0))(x)\n",
    "\n",
    "# spectralLoss(Model.model, pred_yFull, expFactor = 21,spectralTarget = Model.trainingParameters.spectralTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v,g = jax.value_and_grad(spectralLoss)(Model.model, pred_yFull, expFactor = 21,spectralTarget =Model.trainingParameters.spectralTarget)\n",
    "# print(v)\n",
    "# print(g.layers[1].weights[1:20])\n",
    "# g.layers[1].weights.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together all the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Setup optimizer\n",
    "MoAFactor = 0.1\n",
    "L2beta = 1e-8\n",
    "spectralFactor = 1e-3\n",
    "ligandFactor = 1e-5 #0.1 in toy\n",
    "stateLossFactor = 1e-4\n",
    "\n",
    "@jax.jit\n",
    "def criterion(pred_y, y):\n",
    "    \n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "#@jax.jit\n",
    "def criterion_mat(pred_y, y):\n",
    "    # another version, can be called with different types of input.\n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "@jax.jit\n",
    "def simulate(model,x):\n",
    "    return jax.vmap(model, out_axes=(0,0))(x)\n",
    "\n",
    "filter_spec = jtu.tree_map(lambda _: False, Model.model)\n",
    "filter_spec = eqx.tree_at(\n",
    "    lambda tree: (tree.layers[1].weights, tree.layers[1].biases),\n",
    "    filter_spec,\n",
    "    replace=(True, True),\n",
    ")\n",
    "\n",
    "# jax/optax alternative: \n",
    "#! @eqx.filter_jit\n",
    "@ft.partial(eqx.filter_value_and_grad, arg=filter_spec)\n",
    "# @eqx.filter_value_and_grad\n",
    "def compute_loss(model, x, y, dataIndex, currentState):\n",
    "    \n",
    "    (pred_y, pred_yFull) = simulate(model,x)\n",
    "\n",
    "    currentState = currentState.at[dataIndex,1,:].set(jnp.squeeze(pred_yFull))\n",
    "\n",
    "    # Avlant adds random noise between input layer and the recurrent layer. \n",
    "    fitLoss = criterion(pred_y,y)\n",
    "\n",
    "    # signConstraints\n",
    "    violated_weights = model.layers[1].getViolations(model.layers[1].weights)\n",
    "    signConstraint = MoAFactor * jnp.sum(jnp.abs(jnp.where(violated_weights,model.layers[1].weights,0)))\n",
    "    \n",
    "    # ligand constraints: \n",
    "    ligandConstraint = ligandFactor * jnp.sum(jnp.square(model.layers[1].biases[model.layers[0].inOutIndices,0]))\n",
    "    \n",
    "    # State constraints: \n",
    "    stateLoss = stateLossFactor * uniformLoss(currentState, dataIndex, pred_yFull, maxConstraintFactor = 50)\n",
    "\n",
    "    # Parameter costraints: \n",
    "    biasLoss = L2beta * jnp.sum(jnp.square(model.layers[1].biases))\n",
    "    weightLoss = L2beta * jnp.sum(jnp.square(model.layers[1].weights))\n",
    "    \n",
    "    spectralRadiusLoss = spectralFactor * spectralLoss(model, pred_yFull, expFactor = 21)\n",
    "\n",
    "    # sum of constraints:\n",
    "    # loss = fitLoss + signConstraint + ligandConstraint + weightLoss + biasLoss + spectralFactor * spectralRadiusLoss + stateLoss\n",
    "    loss = fitLoss + signConstraint + ligandConstraint + weightLoss + biasLoss + stateLoss + spectralRadiusLoss \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ft.partial(jax.jit, static_argnums=1)\n",
    "\n",
    "#g = eqx.filter_value_and_grad(compute_loss1,has_aux=True)\n",
    "#fg = ft.partial(g,arg=filter_spec)\n",
    "\n",
    "\n",
    "#@ft.partial(eqx.filter_value_and_grad, arg=filter_spec, has_aux=True)\n",
    "def compute_loss1(model, x, y,  dataIndex, currentState):\n",
    "    \n",
    "    (pred_y, pred_yFull) = simulate(model,x)\n",
    "    #model = eqx.combine(params, static)\n",
    "    currentState = currentState.at[dataIndex,1,:].set(jnp.squeeze(pred_yFull))\n",
    "\n",
    "    # Avlant adds random noise between input layer and the recurrent layer. \n",
    "    fitLoss = criterion(pred_y,y)\n",
    "\n",
    "    # signConstraints\n",
    "    violated_weights = model.layers[1].getViolations(model.layers[1].weights)\n",
    "    signConstraint = MoAFactor * jnp.sum(jnp.abs(jnp.where(violated_weights,model.layers[1].weights,0)))\n",
    "    \n",
    "    # ligand constraints: \n",
    "    ligandConstraint = ligandFactor * jnp.sum(jnp.square(model.layers[1].biases[model.layers[0].inOutIndices,0]))\n",
    "    \n",
    "    # State constraints: \n",
    "    stateLoss = stateLossFactor * uniformLoss(currentState, dataIndex, pred_yFull, maxConstraintFactor = 50)\n",
    "\n",
    "    # Parameter costraints: \n",
    "    biasLoss = L2beta * jnp.sum(jnp.square(model.layers[1].biases))\n",
    "    weightLoss = L2beta * jnp.sum(jnp.square(model.layers[1].weights))\n",
    "    \n",
    "    # sum of constraints:\n",
    "    # loss = fitLoss + signConstraint + ligandConstraint + weightLoss + biasLoss + spectralFactor * spectralRadiusLoss + stateLoss\n",
    "    loss = fitLoss + signConstraint + ligandConstraint + weightLoss + biasLoss + stateLoss \n",
    "\n",
    "    return loss, pred_yFull\n",
    "\n",
    "dec = ft.partial(eqx.filter_value_and_grad,arg=filter_spec,has_aux=True)\n",
    "#dec_compute_loss1 = jax.jit(dec(compute_loss1))\n",
    "dec_compute_loss1 = dec(compute_loss1)\n",
    "\n",
    "\n",
    "@ft.partial(eqx.filter_value_and_grad, arg=filter_spec)\n",
    "# @eqx.filter_value_and_grad\n",
    "def compute_loss2(model, pred_yFull):\n",
    "    \n",
    "    spectralRadiusLoss = spectralFactor * spectralLoss(model, pred_yFull, expFactor = 21)\n",
    "\n",
    "    return spectralRadiusLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step(model, x, y, opt_state, dataIndex, currentState):\n",
    "    \n",
    "    # model = eqx.combine(params, static)\n",
    "    #(pred_y, pred_yFull) = simulate(model,x)\n",
    "    \n",
    "\n",
    "    (loss1, pred_yFull), grads1 = dec_compute_loss1(model, x, y, dataIndex, currentState)\n",
    "    loss2, grads2 = compute_loss2(model, pred_yFull)\n",
    "    \n",
    "    # detect if there is nan value in the gradients:\n",
    "    if jnp.any(jnp.isnan(grads1.layers[1].weights)):\n",
    "        \n",
    "        num_of_nans = jnp.sum(jnp.isnan(grads1.layers[1].weights))\n",
    "        print(\"nan in gradients of loss1: \" )\n",
    "        print(num_of_nans)\n",
    "        raise Exception(\"nan in gradients of loss1\")\n",
    "\n",
    "    if jnp.any(jnp.isnan(grads2.layers[1].weights)):\n",
    "        num_of_nans = jnp.sum(jnp.isnan(grads2.layers[1].weights))\n",
    "        print(\"nan in gradients of loss2: \")\n",
    "        print(num_of_nans)\n",
    "        raise Exception(\"nan in gradients of loss2\")\n",
    "\n",
    "    #print(\"grads:\")\n",
    "    #print(grads.layers[1].weights)\n",
    "    new_grads = jax.tree_map(lambda x, y: x+y, grads1, grads2)\n",
    "    new_loss = loss1 + loss2\n",
    "\n",
    "    #loss_old, grads_old = compute_loss(model, x, y, dataIndex, currentState)\n",
    "\n",
    "    # print(\"loss diff:\")\n",
    "    # print(loss - loss_old)\n",
    "\n",
    "    # print(\"grads diff:\")\n",
    "    # print(jnp.linalg.norm(grads.layers[1].weights - grads_old.layers[1].weights)/jnp.linalg.norm(grads_old.layers[1].weights))\n",
    "\n",
    "\n",
    "    updates, opt_state = optim.update(new_grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return new_loss, model, opt_state, currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_loss_vg = jax.value_and_grad(compute_loss,has_aux=True)\n",
    "\n",
    "# we have to use the partitioning trick for the make_step\n",
    "#! @ft.partial(jax.jit, static_argnums=1)\n",
    "def make_step_old(model, x, y, opt_state, dataIndex, currentState):\n",
    "    \n",
    "    loss, grads = compute_loss(model, x, y, dataIndex, currentState)\n",
    "    \n",
    "    #print(\"grads:\")\n",
    "    #print(grads.layers[1].weights)\n",
    "    \n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state, currentState\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneCycle(e, maxIter, maxHeight = 1e-3, startHeight=1e-5, endHeight=1e-5, minHeight = 1e-7, peak = 1000):\n",
    "    phaseLength = 0.95 * maxIter\n",
    "    if e<=peak:\n",
    "        effectiveE = e/peak\n",
    "        lr = (maxHeight-startHeight) * 0.5 * (np.cos(np.pi*(effectiveE+1))+1) + startHeight\n",
    "    elif e<=phaseLength:\n",
    "        effectiveE = (e-peak)/(phaseLength-peak)\n",
    "        lr = (maxHeight-endHeight) * 0.5 * (np.cos(np.pi*(effectiveE+2))+1) + endHeight\n",
    "    else:\n",
    "        lr = endHeight\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, l=0.20176, r=0.00001, v=0\n",
      "i=100, l=0.19464, r=0.00006, v=0\n",
      "i=200, l=0.15572, r=0.00020, v=0\n",
      "i=300, l=0.08540, r=0.00042, v=0\n",
      "i=400, l=0.05925, r=0.00070, v=0\n",
      "i=500, l=0.05085, r=0.00100, v=0\n",
      "i=600, l=0.03634, r=0.00131, v=0\n",
      "Backward fail (eigs left returned different eigenvalue)\n",
      "i=700, l=0.02174, r=0.00159, v=1\n",
      "i=800, l=0.01603, r=0.00181, v=1\n",
      "i=900, l=0.01218, r=0.00195, v=1\n"
     ]
    }
   ],
   "source": [
    "optim = optax.inject_hyperparams(optax.adam)(learning_rate=1)\n",
    "opt_state = optim.init(Model.model)\n",
    "\n",
    "resetState = opt_state # state is a tuple, copy it by assignmment\n",
    "# params, static = eqx.partition(Model.model, eqx.is_array)\n",
    "model = Model.model\n",
    "maxIter = 1000\n",
    "batch_size = 5\n",
    "loader_key = jax.random.PRNGKey(142)\n",
    "loader_key, statekey = jax.random.split(loader_key)\n",
    "\n",
    "progress = nn_models.rnnModel.OptimProgress(maxIter)\n",
    "\n",
    "currentState = jax.random.uniform(shape=(N, 1, Model.model.layers[1].biases.shape[0]),key=statekey)\n",
    "\n",
    "for e in range(maxIter):\n",
    "\n",
    "    currentLoss = []\n",
    "    learning_rate_e = oneCycle(e, maxIter, maxHeight = 2e-3, minHeight = 1e-8, peak = 1000)\n",
    "    opt_state.hyperparams['learning_rate'] = learning_rate_e\n",
    "    \n",
    "    curLoss = []\n",
    "    curEig = []\n",
    "    iter_data = dataIndexloader((X, Y), batch_size, key=loader_key)\n",
    "\n",
    "    for dataIndex in iter_data:\n",
    "        x = X[dataIndex, :]\n",
    "        y = Y[dataIndex, :]\n",
    "\n",
    "        loss, model, opt_state, currentState = make_step(model, x, y, opt_state, dataIndex, currentState)\n",
    "        \n",
    "        currentLoss.append(loss.item())\n",
    "\n",
    "    #fitLoss = compute_test(params, static, Xtest, Ytest)\n",
    "    trained_model = model #eqx.combine(params, static)\n",
    "    \n",
    "\n",
    "    # compute progress on the validation set:     \n",
    "    #Yhat = jax.vmap(trained_model)(Xtest)\n",
    "    #fitLoss = criterion_mat(Yhat,Ytest)\n",
    "    \n",
    "    progress.stats['violations'][e] = np.sum(trained_model.layers[1].getViolations()).item()\n",
    "    #progress.stats['test'][e] = fitLoss.item()\n",
    "    progress.storeProgress(loss=currentLoss, lr=learning_rate_e, violations=np.sum(trained_model.layers[1].getViolations(trained_model.layers[1].weights)).item())\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        progress.printStats(e)\n",
    "\n",
    "    if np.logical_and(e % 100 == 0, e>0):\n",
    "        opt_state = resetState\n",
    "\n",
    "\n",
    "# pytorch version finished 10 000 iteration in 7:52 mins\n",
    "# i=9900, l=0.00012, s=0.718, r=0.00001, v=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.loadtxt(\"M.csv\", delimiter=\",\")\n",
    "net = np.loadtxt(\"ind.csv\", delimiter=\",\").astype(int)\n",
    "w = np.loadtxt(\"w.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "M[net[0],net[1]] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.775542013347149"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(M),axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37025019316109226"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e,v = np.linalg.eig(M)\n",
    "max(abs(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax_ode')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bd868a791ae3f2e25c037fe0842082b59576b23b402251a9a0f392799515c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
