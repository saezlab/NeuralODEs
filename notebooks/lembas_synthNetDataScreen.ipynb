{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to reproduce the `Model/synthNetDataScreen.py` script from Avlant's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from nn_cno import nn_models\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import jax.tree_util as jtu\n",
    "from jax.experimental import sparse\n",
    "from jax import numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import functools as ft\n",
    "\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was run on the cluster. The args were found in the `sunSlurmSynthScreen.sh` file. It takes a value between 0 and 14. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to evaluate the different parts on a cluster. \n",
    "\n",
    "# #Get data number\n",
    "# parser = argparse.ArgumentParser(prog='Macrophage simulation')\n",
    "# parser.add_argument('--selectedCondition', action='store', default=None)\n",
    "# args = parser.parse_args()\n",
    "# curentId = int(args.selectedCondition)\n",
    "\n",
    "currentID = 0     # goes between 0-14 based on the sunSlurmSynthScreen.sh file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testCondtions = pd.read_csv('synthNetScreen/conditions.tsv', sep='\\t', low_memory=False)\n",
    "simultaniousInput = int(testCondtions.loc[currentID == testCondtions['Index'],:]['Ligands'].values)\n",
    "N = int(testCondtions.loc[currentID == testCondtions['Index'],:]['DataSize'].values)\n",
    "print(currentID, simultaniousInput, N)\n",
    "\n",
    "inputAmplitude = 3\n",
    "projectionAmplitude = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the model using the new implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFile = \"data/KEGGnet-Model.tsv\"\n",
    "annotationFile = 'data/KEGGnet-Annotation.tsv'\n",
    "parameterFile = 'synthNetScreen/equationParams.txt'\n",
    "\n",
    "parameterizedModel = nn_models.bioNetwork(networkFile=modelFile, \n",
    "                                          nodeAnnotationFile=annotationFile,\n",
    "                                          inputAmplitude=inputAmplitude,\n",
    "                                          projectionAmplitude=projectionAmplitude)\n",
    "parameterizedModel.loadParams(parameterFile)\n",
    "\n",
    "Model = nn_models.bioNetwork(networkFile=modelFile, nodeAnnotationFile=annotationFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-silico data was generated by simulating the model with given parameters.\n",
    "I think the in-silico model was a result of some kind of optimization, because the paper\n",
    "mentions that the random parameterization results in pretty uniform output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "X = np.zeros((N, len(parameterizedModel.network.inName)))\n",
    "for i in range(1, N): #skip 0 to include a ctrl sample i.e. zero input\n",
    "    X[i, (i-1) % len(parameterizedModel.network.inName)] = np.random.rand(1) #stimulate each receptor at least once\n",
    "    X[i, np.random.randint(0, len(parameterizedModel.network.inName), simultaniousInput-1)] = np.random.rand(simultaniousInput-1)\n",
    "# compared X with pytorch version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code the `Yfull` is used for regularization. This is the state variable after the recurrent NN layer, but before the output layer. It contains the state of all the proteins in the network. \n",
    "\n",
    "rewrote the `call` function of the model to return both, but was not  sure how to use vmap if there are multiple outputs, but it seems simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:(101,)\n",
      "(10, 1, 55)\n",
      "(10, 1, 409)\n",
      "input shape:(101,)\n",
      "(10, 1, 55)\n"
     ]
    }
   ],
   "source": [
    "controlIndex = 0\n",
    "# Y, YfullRef = parameterizedModel(X)\n",
    "Y, YfullRef = jax.vmap(parameterizedModel.model, in_axes=(0),out_axes=(0,0))(X)\n",
    "print(Y.shape)\n",
    "print(YfullRef.shape)\n",
    "\n",
    "\n",
    "\n",
    "Y_old = jax.vmap(parameterizedModel.model.call_old, in_axes=(0),out_axes=(0))(X)\n",
    "print(Y_old.shape)\n",
    "assert((Y_old == Y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup optimizer\n",
    "noiseLevel = 10\n",
    "batchSize = 5\n",
    "MoAFactor = 0.1\n",
    "spectralFactor = 1e-3\n",
    "maxIter = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the `dataloader_inf` for the toy model. It reuses the training data if the batch `size * n_steps` is larger than the available training data. \n",
    "\n",
    "but we should use the `dataloader_finite` for the comparison with Avlant's approach. This use each data only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch has a data loader, but jax does not.\n",
    "# trainloader = torch.utils.data.DataLoader(range(N), batch_size=batchSize, shuffle=True)\n",
    "\n",
    "# here we take the example from https://docs.kidger.site/equinox/examples/train_rnn/\n",
    "# from equinox\n",
    "\n",
    "# this is another version of dataloader, which can be emptied. \n",
    "import jax.random as jrandom\n",
    "\n",
    "# \n",
    "def dataloader_inf(arrays, batch_size, *, key):\n",
    "    \"\"\" Create an infinite dataloader from data. \n",
    "    \n",
    "    It goes through the data and returns batches of size batch_size.\n",
    "    When it finishes, it starts again from the beginning.\n",
    "\n",
    "    Args:\n",
    "        arrays: list of arrays to be batched.\n",
    "        batch_size: size of the batch.\n",
    "        key: random key.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "def dataloader_finite(arrays, batch_size, *, key):\n",
    "    \"\"\" Create an finite dataloader from data. \n",
    "    \n",
    "    It goes through the data and returns batches of size batch_size, finally, \n",
    "    it returns the last batch if the data is not divisible by batch_size.\n",
    "\n",
    "    Args:\n",
    "        arrays: list of arrays to be batched.\n",
    "        batch_size: size of the batch.\n",
    "        key: random key.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "\n",
    "    perm = jrandom.permutation(key, indices)\n",
    "    (key,) = jrandom.split(key, 1)\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    while end < dataset_size:\n",
    "        batch_perm = perm[start:end]\n",
    "        yield tuple(array[batch_perm] for array in arrays)\n",
    "        start = end\n",
    "        end = start + batch_size\n",
    "    \n",
    "    if start < dataset_size:\n",
    "        batch_perm = perm[start:]\n",
    "        yield tuple(array[batch_perm] for array in arrays)\n",
    "\n",
    "\n",
    "def dataIndexloader(arrays, batch_size, *, key):\n",
    "    \"\"\" Create the indices used in the batch.\n",
    "    \n",
    "    Args:\n",
    "        arrays: list of arrays to be batched.\n",
    "        batch_size: size of the batch.\n",
    "        key: random key.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "\n",
    "    perm = jrandom.permutation(key, indices)\n",
    "    (key,) = jrandom.split(key, 1)\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    while end < dataset_size:\n",
    "        batch_perm = perm[start:end]\n",
    "        yield batch_perm\n",
    "        start = end\n",
    "        end = start + batch_size\n",
    "    \n",
    "    if start < dataset_size:\n",
    "        batch_perm = perm[start:]\n",
    "        yield batch_perm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how to rewrite the `uniformLoss` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformLoss(curState, dataIndex, YhatFull, targetMin = 0, targetMax = 0.99, maxConstraintFactor = 10):\n",
    "    #data = curState.detach().clone()\n",
    "    #data[dataIndex, :] = YhatFull\n",
    "    loss = uniformLossBatch(curState, targetMin = targetMin, targetMax = targetMax, maxConstraintFactor = maxConstraintFactor)\n",
    "    return loss\n",
    "\n",
    "def uniformLossBatch(YhatFull, targetMin = 0, targetMax = 0.99, maxConstraintFactor = 10):\n",
    "    targetMean = (targetMax-targetMin)/2\n",
    "    targetVar = (targetMax-targetMin)**2/12\n",
    "\n",
    "    factor = 1\n",
    "    meanFactor = factor\n",
    "    varFactor = factor\n",
    "    minFactor = factor\n",
    "    maxFactor = factor\n",
    "    maxConstraintFactor = factor * maxConstraintFactor\n",
    "\n",
    "    nodeMean = jnp.mean(YhatFull, axis=0)\n",
    "    nodeVar = jnp.mean(jnp.square(YhatFull-nodeMean), axis=0)\n",
    "    maxVal = jnp.amax(YhatFull, axis=0)\n",
    "    minVal = jnp.amin(YhatFull, axis=0)\n",
    "\n",
    "    meanLoss = meanFactor * jnp.sum(jnp.square(nodeMean - targetMean))\n",
    "    varLoss =  varFactor * jnp.sum(jnp.square(nodeVar - targetVar))\n",
    "    maxLoss = maxFactor * jnp.sum(jnp.square(maxVal - targetMax))\n",
    "    minloss = minFactor * jnp.sum(jnp.square(minVal- targetMin))\n",
    "    maxConstraint = -maxConstraintFactor * jnp.sum(maxVal[maxVal<=0]) #max value should never be negative\n",
    "\n",
    "    loss = meanLoss + varLoss + minloss + maxLoss + maxConstraint\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing `uniformLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "maxIter = 100\n",
    "loader_key = jax.random.PRNGKey(142)\n",
    "progress = nn_models.rnnModel.OptimProgress(maxIter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 101)\n",
      "input shape:(101,)\n",
      "(10, 1, 409)\n",
      "(5, 1, 409)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.05038045, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_data = dataIndexloader((X, Y), batch_size, key=loader_key)\n",
    "dataIndex = next(iter_data)\n",
    "x = X[dataIndex, :]\n",
    "y = Y[dataIndex, :]\n",
    "print(x.shape)\n",
    "(pred_y, pred_yFull) = jax.vmap(Model.model, out_axes=(0,0))(x)\n",
    "#currentState = jax.random.uniform(shape=(N, Model.model.layers[1].biases.shape[0]),key=statekey)\n",
    "currentState = jnp.zeros(shape=(N, 1, Model.model.layers[1].biases.shape[0]))\n",
    "\n",
    "print(currentState.shape)\n",
    "print(pred_yFull.shape)\n",
    "stateLossFactor = 1e-4\n",
    "\n",
    "currentState = currentState.at[dataIndex,1,:].set(jnp.squeeze(pred_yFull))\n",
    "\n",
    "# State constraints: \n",
    "stateLoss = stateLossFactor * uniformLoss(currentState, dataIndex, pred_yFull, maxConstraintFactor = 50)\n",
    "stateLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking spectral loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def oneStepDeltaActivationFactor(x, leak):\n",
    "    \"\"\" derivative of the activation function F(x):\n",
    "        x < 0:        leak \n",
    "        0 < x < 0.5 : 1\n",
    "        0.5 < x :      0.25/x^2\n",
    "    \"\"\"\n",
    "    y = jnp.ones(x.shape[1]) #derivative = 1 if nothing else is stated\n",
    "    y = jnp.where(x <= 0, leak, y)  #let derivative be 0.01 at x=0\n",
    "    \n",
    "    y = jnp.where(x > 0.5, 0.25/(x**2), y)\n",
    "    return y\n",
    "\n",
    "def lreig(A):\n",
    "    # for DENSE Matrix\n",
    "    #fall back if eigs fails\n",
    "    e, w, v = sp.linalg.eig(A, left = True)\n",
    "    selected = np.argmax(np.abs(e))\n",
    "    eValue = e[selected]\n",
    "    # selected = (e == eValue)\n",
    "\n",
    "    # if numpy.sum(selected) == 1:\n",
    "    w = w[:,selected]\n",
    "    v = v[:,selected]\n",
    "    # else:\n",
    "    #     w = numpy.sum(w[:,selected], axis=1, keepdims=True)\n",
    "    #     v = numpy.sum(v[:,selected], axis=1, keepdims=True)\n",
    "    #     w = w/norm(w)\n",
    "    #     v = v/norm(v)\n",
    "    return eValue, v, w\n",
    "\n",
    "# A: sparse matrix\n",
    "@jax.custom_jvp\n",
    "def getspectralRadius(weights, ind, M):\n",
    "    \n",
    "    A = sp.sparse.csr_matrix((weights, ind), shape=M.shape, dtype='float32')\n",
    "    tolerance = 10**-6\n",
    "\n",
    "    try:\n",
    "        e, v = sp.sparse.linalg.eigs(A, k=1, which='LM', ncv=100, tol = tolerance)\n",
    "        v = v[:,0]\n",
    "        e = e[0]\n",
    "    except  (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "    except:\n",
    "        print('Forward fail (did not find any eigenvalue with eigs)')\n",
    "        tmpA = A.toarray()\n",
    "        e, v, w = lreig(tmpA) #fall back to solving full eig problem\n",
    "\n",
    "    spectralRadius = np.abs(e)\n",
    "    #ctx.e = e\n",
    "    #ctx.v = v\n",
    "    #ctx.w = np.empty(0)\n",
    "\n",
    "    return spectralRadius\n",
    "\n",
    "def getspectralRadius_save_out(weights, ind, M):\n",
    "    \n",
    "    A = sp.sparse.csr_matrix((weights, ind), shape=M.shape, dtype='float32')\n",
    "    tolerance = 10**-6\n",
    "\n",
    "    try:\n",
    "        e, v = sp.sparse.linalg.eigs(A, k=1, which='LM', ncv=100, tol = tolerance)\n",
    "        v = v[:,0]\n",
    "        e = e[0]\n",
    "    except  (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "    except:\n",
    "        print('Forward fail (did not find any eigenvalue with eigs)')\n",
    "        tmpA = A.toarray()\n",
    "        e, v, w = lreig(tmpA) #fall back to solving full eig problem\n",
    "\n",
    "    spectralRadius = np.abs(e)\n",
    "    e = e\n",
    "    v = v\n",
    "    w = np.empty(0)\n",
    "\n",
    "    return spectralRadius, e, v, w\n",
    "\n",
    "@getspectralRadius.defjvp\n",
    "def AgetspectralRadius_jvp(primals, tangents):\n",
    "    weights, ind, M = primals\n",
    "    w_dot, ind_dot, M_dot  = tangents\n",
    "    primal_out, e, v, w = getspectralRadius_save_out(weights, ind, M)\n",
    "\n",
    "    tolerance = 10**-6\n",
    "    networkList = ind\n",
    "    A = sp.sparse.csr_matrix((weights, ind), shape=M.shape, dtype='float32')\n",
    "    \n",
    "    tmpA = A\n",
    "    tmpA = tmpA.T  #tmpA.T.toarray()\n",
    "\n",
    "    if w.shape[0]==0:\n",
    "        try:\n",
    "            eT = e\n",
    "            if np.isreal(eT): #does for some reason not converge if imag = 0\n",
    "                eT = eT.real\n",
    "            e2, w = sp.sparse.linalg.eigs(tmpA, k=1, sigma=eT, OPpart='r', tol=tolerance)\n",
    "            selected = 0 #numpy.argmin(numpy.abs(e2-eT))\n",
    "            w = w[:,selected]\n",
    "            e2 = e2[selected]\n",
    "            #Check if same eigenvalue\n",
    "            if abs(e-e2)>(tolerance*10):\n",
    "                print('Backward fail (eigs left returned different eigenvalue)')\n",
    "                w = np.empty(0)\n",
    "                #e, v, w = lreig(tmpA) #fall back to solving whole eig problem\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            print('Backward fail (did not find any eigenvalue with eigs)')\n",
    "            #e, v, w = lreig(tmpA) #fall back to solving full eig problem\n",
    "            delta = np.zeros(weights.shape)\n",
    "\n",
    "\n",
    "    if w.shape[0] != 0:\n",
    "        divisor = w.T.dot(v).flatten()\n",
    "        if abs(divisor) == 0:\n",
    "            delta = np.zeros(weights.shape)\n",
    "            print('Empty eig')\n",
    "        else:\n",
    "            delta = np.multiply(w[networkList[0]], v[networkList[1]])/divisor\n",
    "            direction = e/np.abs(e)\n",
    "            delta = (delta/direction).real\n",
    "    else:\n",
    "        #print('Empty eig')\n",
    "        delta = np.zeros(weights.shape)\n",
    "\n",
    "    #deltaFilter = numpy.not_equal(numpy.sign(delta), numpy.sign(ctx.weights))\n",
    "    #delta[deltaFilter] = 0\n",
    "\n",
    "    #delta = torch.tensor(delta, dtype = grad_output.dtype)\n",
    "\n",
    "    constrainNorm = True\n",
    "    if constrainNorm:\n",
    "        norm = np.linalg.norm(delta)\n",
    "        if norm>10:\n",
    "            delta = delta/norm #typical seems to be ~0.36\n",
    "        #delta = delta * numpy.abs(ctx.weights)\n",
    "        #delta = delta/norm(delta)\n",
    "\n",
    "    tangent_out = jnp.dot(delta,w_dot)\n",
    "\n",
    "    return primal_out, tangent_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@jax.jit\n",
    "def spectralLoss(model, YhatFull, expFactor = 20, lb=0.5, spectralTarget =0.95):\n",
    "    \"\"\" spectral loss of the transmission function:\n",
    "    \n",
    "    check Fig3, panel A\n",
    "    derivativeOfActivation(steadystate) * A \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"spectral loss compiled\")\n",
    "\n",
    "    # selects a random condition to be used in the spectral computation:\n",
    "    randomIndex = np.random.randint(YhatFull.shape[0])\n",
    "\n",
    "    # evaluate the derivative of the activation function at the steady state:\n",
    "    activationFactor = oneStepDeltaActivationFactor(YhatFull[randomIndex,:], model.layers[1].leak)\n",
    "    weightFactor = activationFactor.flatten()[model.layers[1].networkList[0]]\n",
    "    multipliedWeightFactor =  model.layers[1].weights * weightFactor\n",
    "    \n",
    "    M = jnp.zeros(shape=model.layers[1].A.shape)\n",
    "    ind = model.layers[1].networkList\n",
    "    \n",
    "    spectralRadius = getspectralRadius(multipliedWeightFactor, ind, M)\n",
    "    \n",
    "    scaleFactor = 1/jnp.exp(expFactor *  spectralTarget)\n",
    "    \n",
    "    #spectralRadiusLoss = scaleFactor * spectralRadius\n",
    "    #spectralRadiusLoss = scaleFactor * (jnp.exp(expFactor*spectralRadius)-1)\n",
    "\n",
    "    # https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where\n",
    "    spectralRadiusLoss = jnp.where(spectralRadius > lb, \n",
    "                                   scaleFactor * (jnp.exp(expFactor*spectralRadius)-1),\n",
    "                                   jnp.zeros(1))[0]\n",
    "    # if spectralRadius>lb:\n",
    "    #     spectralRadiusLoss = scaleFactor * (jnp.exp(expFactor*spectralRadius)-1)\n",
    "    # else:\n",
    "    #     spectralRadiusLoss = 0.0\n",
    "\n",
    "    return spectralRadiusLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:(101,)\n",
      "spectral loss compiled\n"
     ]
    },
    {
     "ename": "TracerArrayConversionError",
     "evalue": "The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int32[832])>with<DynamicJaxprTrace(level=0/2)>\nWhile tracing the function spectralLoss at /var/folders/cx/9kyr3rt90c974wdygym_lhgh0000gn/T/ipykernel_61193/747990753.py:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'model'.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m M \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mzeros(shape \u001b[39m=\u001b[39m Model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mA\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m (pred_y, pred_yFull) \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(Model\u001b[39m.\u001b[39mmodel, out_axes\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m))(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m spectralLoss(Model\u001b[39m.\u001b[39;49mmodel, pred_yFull, expFactor \u001b[39m=\u001b[39;49m \u001b[39m21\u001b[39;49m,spectralTarget \u001b[39m=\u001b[39;49m Model\u001b[39m.\u001b[39;49mtrainingParameters\u001b[39m.\u001b[39;49mspectralTarget)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb Cell 23\u001b[0m in \u001b[0;36mspectralLoss\u001b[0;34m(model, YhatFull, expFactor, lb, spectralTarget)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m M \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mzeros(shape\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mA\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m ind \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnetworkList\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m spectralRadius \u001b[39m=\u001b[39m getspectralRadius(multipliedWeightFactor, ind, M)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m scaleFactor \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mjnp\u001b[39m.\u001b[39mexp(expFactor \u001b[39m*\u001b[39m  spectralTarget)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#spectralRadiusLoss = scaleFactor * spectralRadius\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#spectralRadiusLoss = scaleFactor * (jnp.exp(expFactor*spectralRadius)-1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb Cell 23\u001b[0m in \u001b[0;36mgetspectralRadius\u001b[0;34m(weights, ind, M)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mcustom_jvp\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetspectralRadius\u001b[39m(weights, ind, M):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     A \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mcsr_matrix((weights, ind), shape\u001b[39m=\u001b[39;49mM\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     tolerance \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m6\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X65sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/jax_ode/lib/python3.10/site-packages/scipy/sparse/_compressed.py:53\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(arg1) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     51\u001b[0m         \u001b[39m# (data, ij) format\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(\n\u001b[0;32m---> 53\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coo_container(arg1, shape\u001b[39m=\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m     54\u001b[0m         )\n\u001b[1;32m     55\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_self(other)\n\u001b[1;32m     56\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(arg1) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m     57\u001b[0m         \u001b[39m# (data, indices, indptr) format\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/jax_ode/lib/python3.10/site-packages/scipy/sparse/_coo.py:158\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape \u001b[39m=\u001b[39m check_shape((M, N))\n\u001b[1;32m    157\u001b[0m idx_dtype \u001b[39m=\u001b[39m get_index_dtype(maxval\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape))\n\u001b[0;32m--> 158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(row, copy\u001b[39m=\u001b[39;49mcopy, dtype\u001b[39m=\u001b[39;49midx_dtype)\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(col, copy\u001b[39m=\u001b[39mcopy, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m getdata(obj, copy\u001b[39m=\u001b[39mcopy, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/jax_ode/lib/python3.10/site-packages/jax/core.py:537\u001b[0m, in \u001b[0;36mTracer.__array__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m--> 537\u001b[0m   \u001b[39mraise\u001b[39;00m TracerArrayConversionError(\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int32[832])>with<DynamicJaxprTrace(level=0/2)>\nWhile tracing the function spectralLoss at /var/folders/cx/9kyr3rt90c974wdygym_lhgh0000gn/T/ipykernel_61193/747990753.py:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'model'.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError"
     ]
    }
   ],
   "source": [
    "# Test the spectral loss function\n",
    "iter_data = dataIndexloader((X, Y), batch_size, key=loader_key)\n",
    "dataIndex = next(iter_data)\n",
    "x = X[dataIndex, :]\n",
    "y = Y[dataIndex, :]\n",
    "M = jnp.zeros(shape = Model.model.layers[1].A.shape)\n",
    "\n",
    "(pred_y, pred_yFull) = jax.vmap(Model.model, out_axes=(0,0))(x)\n",
    "\n",
    "spectralLoss(Model.model, pred_yFull, expFactor = 21,spectralTarget = Model.trainingParameters.spectralTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectral loss compiled\n",
      "6.831739e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 1.04067749e-13,  3.01219045e-14, -1.19719213e-16,\n",
       "              4.39322770e-17,  1.30915325e-07,  5.25309861e-06,\n",
       "              3.13666715e-06, -3.93654389e-08,  3.93511891e-06,\n",
       "              5.26099448e-15, -2.22454712e-08, -1.61918603e-08,\n",
       "             -1.00334593e-11, -9.72264935e-09, -7.02504055e-10,\n",
       "             -7.07684533e-09, -4.38524304e-12, -3.90130282e-07,\n",
       "             -2.53217669e-09, -6.85135074e-11, -3.11517612e-09,\n",
       "             -6.05953403e-07, -2.44680471e-08, -3.65580307e-21,\n",
       "              2.77452926e-14, -3.16464963e-07, -5.34783145e-16,\n",
       "             -7.64424704e-11,  6.10337247e-06,  8.33020181e-11,\n",
       "              3.45965788e-07,  9.09419415e-11,  2.22807744e-10,\n",
       "              3.97434206e-07, -1.79171948e-15, -7.10606644e-16,\n",
       "              5.87045739e-14, -1.39061841e-12,  6.47419726e-13,\n",
       "              1.02864556e-06,  4.25671244e-13, -2.31076578e-13,\n",
       "             -1.38507344e-16, -2.81680366e-14,  4.67178921e-14,\n",
       "              4.45526557e-14,  1.04642902e-06, -1.09227406e-12,\n",
       "              2.79799892e-13,  6.21042795e-14, -1.80168624e-13,\n",
       "              4.79888210e-13,  3.63803743e-09, -5.53119426e-14,\n",
       "              8.53367862e-11, -4.08970492e-13,  2.76349202e-13,\n",
       "             -6.59974950e-13, -4.40064956e-14,  1.23604772e-12,\n",
       "             -7.81655082e-14,  5.39155817e-14,  7.76410889e-06,\n",
       "              1.12678176e-14, -1.70359300e-12,  7.22910772e-13,\n",
       "              8.22628351e-07, -1.03442775e-12, -1.60477870e-16,\n",
       "             -7.52400097e-16, -8.74035934e-16, -2.06695455e-16,\n",
       "             -1.18879486e-15, -2.33016053e-15,  1.89665028e-18,\n",
       "              7.99550980e-16,  6.07860784e-06,  3.09608458e-06,\n",
       "              6.61034378e-13, -2.90100463e-13,  4.29543979e-06,\n",
       "              1.25493034e-06,  3.58632292e-06,  2.53427015e-06,\n",
       "              2.14288161e-06,  3.01852964e-14,  1.97428913e-14,\n",
       "              4.71469042e-14,  5.17819532e-14,  5.47688522e-14,\n",
       "              6.63352633e-14,  7.44922787e-14,  3.76552052e-08,\n",
       "              3.53553133e-08,  3.71102402e-08,  1.89017690e-08,\n",
       "              8.86209008e-08,  1.61486238e-08, -2.23721305e-10],            dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v,g = jax.value_and_grad(spectralLoss)(Model.model, pred_yFull, expFactor = 21,spectralTarget =Model.trainingParameters.spectralTarget)\n",
    "print(v)\n",
    "g.layers[1].weights[1:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together all the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layer: https://docs.kidger.site/equinox/examples/frozen_layer/\n",
    "# We copy the pyTree and indicate that we want to estimate the parmeters only on the recurrent layer. \n",
    "# we use the generated pyTree to filter the gradient. \n",
    "filter_spec = jtu.tree_map(lambda _: False, Model.model)\n",
    "filter_spec = eqx.tree_at(\n",
    "    lambda tree: (tree.layers[1].weights, tree.layers[1].biases),\n",
    "    filter_spec,\n",
    "    replace=(True, True),\n",
    ")\n",
    "#Setup optimizer\n",
    "MoAFactor = 0.1\n",
    "L2beta = 1e-8\n",
    "spectralFactor = 0\n",
    "ligandFactor = 1e-5 #0.1 in toy\n",
    "stateLossFactor = 1e-4\n",
    "\n",
    "#@jax.jit\n",
    "def criterion(pred_y, y):\n",
    "    # Trains with respect to binary cross-entropy\n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "#@jax.jit\n",
    "def criterion_mat(pred_y, y):\n",
    "    # Trains with respect to binary cross-entropy\n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "\n",
    "# jax/optax alternative: \n",
    "#! @eqx.filter_jit\n",
    "# @ft.partial(eqx.filter_value_and_grad, arg=filter_spec)\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model, x, y, dataIndex, currentState):\n",
    "    \n",
    "    (pred_y, pred_yFull) = jax.vmap(model, out_axes=(0,0))(x)\n",
    "\n",
    "    currentState = currentState.at[dataIndex,1,:].set(jnp.squeeze(pred_yFull))\n",
    "\n",
    "    # Avlant adds random noise between input layer and the recurrent layer. \n",
    "    fitLoss = criterion(pred_y,y)\n",
    "\n",
    "    # signConstraints\n",
    "    violated_weights = model.layers[1].getViolations(model.layers[1].weights)\n",
    "    signConstraint = MoAFactor * jnp.sum(jnp.abs(jnp.where(violated_weights,model.layers[1].weights,0)))\n",
    "    \n",
    "    # ligand constraints: \n",
    "    ligandConstraint = ligandFactor * jnp.sum(jnp.square(model.layers[1].biases[model.layers[0].inOutIndices,0]))\n",
    "    \n",
    "    # State constraints: \n",
    "    stateLoss = stateLossFactor * uniformLoss(currentState, dataIndex, pred_yFull, maxConstraintFactor = 50)\n",
    "\n",
    "    # Parameter costraints: \n",
    "    biasLoss = L2beta * jnp.sum(jnp.square(model.layers[1].biases))\n",
    "    weightLoss = L2beta * jnp.sum(jnp.square(model.layers[1].weights))\n",
    "    \n",
    "    spectralRadiusLoss = spectralFactor * spectralLoss(model, pred_yFull, expFactor = 21)\n",
    "\n",
    "    # sum of constraints:\n",
    "    # loss = fitLoss + signConstraint + ligandConstraint + weightLoss + biasLoss + spectralFactor * spectralRadiusLoss + stateLoss\n",
    "    loss = fitLoss + signConstraint + ligandConstraint + weightLoss + biasLoss + stateLoss + spectralRadiusLoss \n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_loss_vg = jax.value_and_grad(compute_loss,has_aux=True)\n",
    "\n",
    "# we have to use the partitioning trick for the make_step\n",
    "#! @ft.partial(jax.jit, static_argnums=1)\n",
    "def make_step(params,static, x, y, opt_state, dataIndex, currentState):\n",
    "    model = eqx.combine(params, static)\n",
    "    loss, grads = compute_loss(model, x, y, dataIndex, currentState)\n",
    "    \n",
    "    #print(\"grads:\")\n",
    "    #print(grads.layers[1].weights)\n",
    "    \n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    params = eqx.apply_updates(params, updates)\n",
    "    return loss, params, opt_state, currentState\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneCycle(e, maxIter, maxHeight = 1e-3, startHeight=1e-5, endHeight=1e-5, minHeight = 1e-7, peak = 1000):\n",
    "    phaseLength = 0.95 * maxIter\n",
    "    if e<=peak:\n",
    "        effectiveE = e/peak\n",
    "        lr = (maxHeight-startHeight) * 0.5 * (np.cos(np.pi*(effectiveE+1))+1) + startHeight\n",
    "    elif e<=phaseLength:\n",
    "        effectiveE = (e-peak)/(phaseLength-peak)\n",
    "        lr = (maxHeight-endHeight) * 0.5 * (np.cos(np.pi*(effectiveE+2))+1) + endHeight\n",
    "    else:\n",
    "        lr = endHeight\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optim \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39minject_hyperparams(optax\u001b[39m.\u001b[39madam)(learning_rate\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m opt_state \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39minit(Model\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gabora/Documents/SaezGroup/LocalGitRepo/NN_cellnopt/NeuralODEs/notebooks/lembas_synthNetDataScreen.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m resetState \u001b[39m=\u001b[39m opt_state \u001b[39m# state is a tuple, copy it by assignmment\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optax' is not defined"
     ]
    }
   ],
   "source": [
    "optim = optax.inject_hyperparams(optax.adam)(learning_rate=1)\n",
    "opt_state = optim.init(Model.model)\n",
    "\n",
    "resetState = opt_state # state is a tuple, copy it by assignmment\n",
    "params, static = eqx.partition(Model.model, eqx.is_array)\n",
    "\n",
    "maxIter = 100\n",
    "batch_size = 5\n",
    "loader_key = jax.random.PRNGKey(142)\n",
    "loader_key, statekey = jax.random.split(loader_key)\n",
    "\n",
    "progress = nn_models.rnnModel.OptimProgress(maxIter)\n",
    "steps = np.round(X.shape[0]/batch_size).astype(int)\n",
    "\n",
    "currentState = jax.random.uniform(shape=(N, 1, Model.model.layers[1].biases.shape[0]),key=statekey)\n",
    "\n",
    "for e in range(maxIter):\n",
    "\n",
    "    currentLoss = []\n",
    "    learning_rate_e = oneCycle(e, maxIter, maxHeight = 2e-3, minHeight = 1e-8, peak = 200)\n",
    "    opt_state.hyperparams['learning_rate'] = learning_rate_e\n",
    "    \n",
    "    curLoss = []\n",
    "    curEig = []\n",
    "    iter_data = dataIndexloader((X, Y), batch_size, key=loader_key)\n",
    "\n",
    "    for dataIndex in iter_data:\n",
    "        x = X[dataIndex, :]\n",
    "        y = Y[dataIndex, :]\n",
    "\n",
    "        loss, params, opt_state, currentState = make_step(params, static, x, y, opt_state, dataIndex, currentState)\n",
    "        \n",
    "        currentLoss.append(loss.item())\n",
    "\n",
    "    #fitLoss = compute_test(params, static, Xtest, Ytest)\n",
    "    trained_model = eqx.combine(params, static)\n",
    "    \n",
    "\n",
    "    # compute progress on the validation set:     \n",
    "    #Yhat = jax.vmap(trained_model)(Xtest)\n",
    "    #fitLoss = criterion_mat(Yhat,Ytest)\n",
    "    \n",
    "    progress.stats['violations'][e] = np.sum(trained_model.layers[1].getViolations()).item()\n",
    "    #progress.stats['test'][e] = fitLoss.item()\n",
    "    progress.storeProgress(loss=currentLoss, lr=learning_rate_e, violations=np.sum(trained_model.layers[1].getViolations(trained_model.layers[1].weights)).item())\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        progress.printStats(e)\n",
    "\n",
    "    if np.logical_and(e % 100 == 0, e>0):\n",
    "        opt_state = resetState\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax_ode')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bd868a791ae3f2e25c037fe0842082b59576b23b402251a9a0f392799515c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
